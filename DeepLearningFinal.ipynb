{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "DeepLearningFinal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rinaschiller/detecting-political-bias-deep-learning/blob/master/DeepLearningFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8P3lkYWLaDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Uc_XQFNH9rf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import itertools\n",
        "from scipy import misc\n",
        "\n",
        "import torch \n",
        "from torch.utils import data\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import json\n",
        "import sys\n",
        "import io\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "#bert_model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words |= set(['', '``', '-', '--'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpi68bOKH9rl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#add directory\n",
        "baseDir = \"drive/My Drive/sem2/DeepLearning/\"\n",
        "baseDir = \"drive/My Drive/dl_final/\"\n",
        "dataDir = \"\"\n",
        "\n",
        "dem = 0\n",
        "rep = 1\n",
        "\n",
        "#number of sentences for each speech. Pad or trim if necessary\n",
        "MAX_SENT = 15 \n",
        "\n",
        "UNK = '<UNK>'\n",
        "PAD = '<PAD>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bWLGztdW0qv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%ls drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DllIOp5V73kk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLCOKR04Q40j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_csv(path, train=False, ypath=None):\n",
        "    data = pd.read_csv(path, names=['text', 'party'])\n",
        "    xs, ys = data['text'], data['party']\n",
        "\n",
        "    return xs, ys\n",
        "\n",
        "def max_speech_len(speeches):\n",
        "  return max([len(sent) for pp in speeches for sent in pp])\n",
        "\n",
        "# Don't lemmatize because bert uses its own tokenization approach\n",
        "#\n",
        "# Speeches are too long for bert to handle, so we break each one down\n",
        "# into an array of sentences\n",
        "def clean(df):\n",
        "    df = df.str.split(' ')\n",
        "    for i, row in enumerate(df, 0):\n",
        "        df[i] = [w for w in row if w not in stop_words]\n",
        "\n",
        "    for i, row in enumerate(df, 0):\n",
        "        para_arr, sent_arr = [], []\n",
        "        for w in row:\n",
        "            if w in ['.', '?', '!']:\n",
        "                sent_arr.append(w)\n",
        "                para_arr.append(sent_arr),\n",
        "                sent_arr = []\n",
        "            else:\n",
        "                sent_arr.append(w)\n",
        "        df[i] = para_arr\n",
        "\n",
        "    return df\n",
        "\n",
        "# tokenize the dataset. encode_plus pads the data to length 512 and adds a masking list\n",
        "# as well as an attention list, but it seems like those are actually only necesary for\n",
        "# using Bert as a model. We're just using it as a feature extractor, so we only need the\n",
        "# encodings.\n",
        "def bert_tokenize(speeches, labels):\n",
        "    dataset, newlabels = [], []\n",
        "    for i, speech in enumerate(speeches, 0):\n",
        "        pp = []\n",
        "        for sent in speech:\n",
        "            sent = tokenizer.encode(\n",
        "                        sent,\n",
        "                        #max_length=512,\n",
        "                        add_special_tokens=True,\n",
        "                        #pad_to_max_length=True\n",
        "                    )\n",
        "            pp.append(torch.tensor(sent).unsqueeze(0))\n",
        "        # exclude speeches that end up with no content at all from removal of stopwords/punctuations\n",
        "        if len(pp) > 0:\n",
        "            # pad and trim speeches so that each one has MAX_SENT sentences.\n",
        "            # a pad sentence is just the start token (101) and the end token (102) with no content.\n",
        "            if len(pp) < MAX_SENT:\n",
        "                pad = [torch.tensor([101, 102]).unsqueeze(0)] * (MAX_SENT - len(pp))\n",
        "                pp = pad + pp\n",
        "            dataset.append(pp[:MAX_SENT])\n",
        "            newlabels.append(1 if labels[i] == 'R' else 0)\n",
        "    return dataset, newlabels\n",
        "\n",
        "\n",
        "class BertifiedDataSet(data.Dataset):\n",
        "    def __init__(self, xs, ys, model):\n",
        "        self.xs = xs\n",
        "        self.ys = ys\n",
        "        self.model = model\n",
        "        self.data_len = len(self.ys)\n",
        "\n",
        "    # Each item in the bert output is going to be a tuple, where the first\n",
        "    # element is the bert representation for every word in a sentence, and the second\n",
        "    # is the final bert representation for the entire sentence\n",
        "    def __getitem__(self, idx):\n",
        "        with torch.set_grad_enabled(False):\n",
        "            # Get BERT outputs for each sentence in the speech\n",
        "            sentences = [self.model(sent.cuda()) for sent in self.xs[idx]]\n",
        "            label = torch.tensor(self.ys[idx]).cuda()\n",
        "\n",
        "            # Get the hidden state for each sentence, making a MAX_LEN x 1 x 768 tensor\n",
        "            seq = torch.stack([hidden for full, hidden in sentences]).cuda()\n",
        "\n",
        "\n",
        "            return seq, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUAb9bnw6TMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the training data from csv\n",
        "# TODO: Get filename from whereever its stored in drive\n",
        "xtrain, ytrain = load_csv(baseDir + 'training_set_strip.csv')\n",
        "\n",
        "# Reduce the dataset by separating tokens, removing stopwords, and splitting\n",
        "# speeches into sentences.\n",
        "xtrain = clean(xtrain)\n",
        "\n",
        "# Convert 2d array of tokens to a 2d array of bert encodings (which are vocab indexes)\n",
        "xtokens, ytrain = bert_tokenize(xtrain, ytrain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHxHvo7YH9sH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "'''train_data = data.TensorDataset(Variable(torch.LongTensor(train_x)), Variable(torch.LongTensor(train_y)))\n",
        "train_loader = data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "dev_data = data.TensorDataset(Variable(torch.LongTensor(dev_x)), Variable(torch.LongTensor(dev_y)))\n",
        "dev_loader = data.DataLoader(dev_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "test_data = data.TensorDataset(Variable(torch.LongTensor(test_x)), Variable(torch.LongTensor(test_y)))\n",
        "test_loader = data.DataLoader(test_data, shuffle=True, batch_size=batch_size)'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y3xbc1t0yDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    print('using CUDA')\n",
        "    bert_model.cuda()\n",
        "\n",
        "bert_model.eval()\n",
        "dataset = BertifiedDataSet(xtokens, ytrain, bert_model)\n",
        "data_loader = data.DataLoader(dataset, batch_size=512, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPJXWdSGXqtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(dataset.xs), len(dataset.ys))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im9hTjYtb-jP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''import time\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "test = next(iter(data_loader))\n",
        "\n",
        "end = time.time()\n",
        "print(end - start)\n",
        "\n",
        "print(test[0].shape, test[1].shape)'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYRGKbhyH9sV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class RNN(nn.Module):    \n",
        "    def __init__(self, vocab, dim, batch_size, h_dim, layers, tag_size, preTrainedWeights):\n",
        "        super(RNN, self).__init__()\n",
        "        \n",
        "        self.layers = layers\n",
        "        self.batch_size = batch_size\n",
        "        self.h_dim = h_dim\n",
        "        self.dim = dim\n",
        "        \n",
        "        self.embedding = nn.Embedding.from_pretrained(preTrainedWeights)\n",
        "        self.gru = nn.GRU(dim, h_dim, self.layers, batch_first=True)\n",
        "        self.fc = nn.Linear(h_dim*MAX_SENT, tag_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "        self.fc.weight = nn.init.xavier_normal_(self.fc.weight, gain = 1.0)\n",
        "        #self.embedding.weight = nn.init.xavier_normal_(self.embedding.weight, gain = 1.0)\n",
        "        self.hidden = self.init_hidden()\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for name, param in self.named_parameters():\n",
        "            if \"weight_hh\" in name:\n",
        "                nn.init.orthogonal_(param.data)\n",
        "            elif \"weight_ih\" in name:\n",
        "                nn.init.xavier_uniform_(param.data)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        xshape = x.shape\n",
        "        x = self.embedding(x)\n",
        "        #print(x.shape)\n",
        "        x, self.hidden = self.gru(x)\n",
        "        #print(x.shape)\n",
        "        x = x.reshape(x.shape[0], x.shape[1]*x.shape[2]) \n",
        "        #print(x.shape)\n",
        "        #print(x.shape)\n",
        "        x = self.fc(x)\n",
        "        #print(x.shape)\n",
        "        x = self.sigmoid(x)\n",
        "        #x = nn.functional.log_softmax(x, dim = 1)\n",
        "        return x\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        return (torch.zeros(1, 1, self.h_dim),\n",
        "                torch.zeros(1, 1, self.h_dim))\n",
        "        \n",
        "\n",
        "# ----- This class is built to work with our data -----\n",
        " class LSTM(nn.Module):    \n",
        "    def __init__(self, dim, h_dim, layers):\n",
        "        super(LSTM, self).__init__()\n",
        "        \n",
        "        self.layers = layers\n",
        "        self.h_dim = h_dim\n",
        "        self.dim = dim\n",
        "        \n",
        "        self.lstm = nn.LSTM(dim, h_dim, self.layers, batch_first=True)\n",
        "        self.fc = nn.Linear(h_dim*MAX_SENT, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "        self.fc.weight = nn.init.xavier_normal_(self.fc.weight, gain = 1.0)\n",
        "        self.hidden = self.init_hidden()\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for name, param in self.named_parameters():\n",
        "            if \"weight_hh\" in name:\n",
        "                nn.init.orthogonal_(param.data)\n",
        "            elif \"weight_ih\" in name:\n",
        "                nn.init.xavier_uniform_(param.data)\n",
        "    \n",
        "    def forward(self, xs):\n",
        "        xs, self.hidden = self.lstm(xs.view(xs.shape[0], xs.shape[1], -1))\n",
        "        xs = xs.reshape(xs.shape[0], xs.shape[1]*xs.shape[2]) \n",
        "\n",
        "        xs = self.fc(xs)\n",
        "\n",
        "        xs = self.sigmoid(xs)\n",
        "        return xs\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        return (torch.zeros(1, 1, self.h_dim),\n",
        "                torch.zeros(1, 1, self.h_dim))\n",
        "    \n",
        "h_dim = 5\n",
        "layers = 1\n",
        "tag_size = 1\n",
        "    \n",
        "rnn = RNN(len(vocabToId), dim, batch_size, h_dim, layers, tag_size, weights)  \n",
        "criterion = torch.nn.BCELoss() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi55pDEnH9sX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filterPreds(preds, labels):\n",
        "    preds = preds[labels > 0]\n",
        "    labels = labels[labels > 0]\n",
        "    \n",
        "    return preds, labels\n",
        "    \n",
        "def train(epochs):    \n",
        "    lossList = []\n",
        "    for e in range(epochs):\n",
        "        l = 0\n",
        "        print(\"epoch: {}\".format(e))\n",
        "        for sent, labels in train_loader:\n",
        "            rnn.zero_grad()\n",
        "            rnn.hidden = rnn.init_hidden()\n",
        "            \n",
        "            preds = rnn(sent)\n",
        "            \n",
        "            #print(preds.shape)\n",
        "            #print(labels.shape)\n",
        "            \n",
        "            #preds, labels = filterPreds(preds, labels)\n",
        "            \n",
        "            loss = criterion(preds, labels.float())\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            l += loss.item()\n",
        "        \n",
        "        lossList.append(l/len(labels))\n",
        "            \n",
        "        if e%5==0:\n",
        "            right = 0\n",
        "            count = 0\n",
        "            for devSent, devLabel in dev_loader:\n",
        "                devOutputs = rnn(devSent)\n",
        "                devPred = (devOutputs > 0.5).reshape(devOutputs.shape[0]).long()\n",
        "                count+= devLabel.size(0)\n",
        "                right+= (devPred == devLabel).sum().item()\n",
        "            accuracy = (right*100)/count\n",
        "            print(\"Iteration: {}. Loss: {}. Accuracy: {}%.\".format(e, loss.item(), accuracy))\n",
        "        \n",
        "    return lossList\n",
        "\n",
        "\n",
        "\n",
        "# ------- This is the training fn I've been using -------\n",
        "def train_final(epochs, loader, lssfn, opt):    \n",
        "    lossList = []\n",
        "    for e in range(epochs):\n",
        "        l = 0\n",
        "        print(\"epoch: {}\".format(e))\n",
        "        for speeches, labels in loader:\n",
        "            opt.zero_grad()\n",
        "            #lstm.hidden = lstm.init_hidden()\n",
        "            \n",
        "            preds = lstm(speeches)\n",
        "            loss = lssfn(preds, labels.float())\n",
        "            \n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            \n",
        "            l += loss.item()\n",
        "        \n",
        "        lossList.append(l/len(labels))\n",
        "            \n",
        "        '''right = 0\n",
        "        count = 0\n",
        "        for devSent, devLabel in dev_loader:\n",
        "            devOutputs = rnn(devSent)\n",
        "            devPred = (devOutputs > 0.5).reshape(devOutputs.shape[0]).long()\n",
        "            count+= devLabel.size(0)\n",
        "            right+= (devPred == devLabel).sum().item()\n",
        "        accuracy = (right*100)/count'''\n",
        "        print(\"Iteration: {}. Loss: {}%.\".format(e, l))\n",
        "        \n",
        "    return lossList"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfG4HphGH9sa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_rate = 0.001\n",
        "epochs = 25\n",
        "\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr_rate)\n",
        "lossList = train(epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcBX7BXgH9se",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "right = 0\n",
        "count = 0\n",
        "for devSent, devLabel in dev_loader:\n",
        "    devOutputs = rnn(devSent)\n",
        "    devPred = (devOutputs > 0.5).reshape(devOutputs.shape[0]).long()\n",
        "    count+= devLabel.size(0)\n",
        "    right+= (devPred == devLabel).sum().item()\n",
        "accuracy = (right*100)/count\n",
        "print(\"Accuracy: {}%.\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_Ei9DZnH9sh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "right = 0\n",
        "count = 0\n",
        "for devSent, devLabel in test_loader:\n",
        "    devOutputs = rnn(devSent)\n",
        "    devPred = (devOutputs > 0.5).reshape(devOutputs.shape[0]).long()\n",
        "    count+= devLabel.size(0)\n",
        "    right+= (devPred == devLabel).sum().item()\n",
        "accuracy = (right*100)/count\n",
        "print(\"Accuracy: {}%.\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKQhBQGKH9sk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(nn.Module):    \n",
        "    def __init__(self, vocab, dim, batch_size, h_dim, layers, tag_size, preTrainedWeights):\n",
        "        super(MLP, self).__init__()\n",
        "        \n",
        "        self.layers = layers\n",
        "        self.batch_size = batch_size\n",
        "        self.h_dim = h_dim\n",
        "        self.dim = dim\n",
        "        \n",
        "        self.embedding = nn.Embedding.from_pretrained(preTrainedWeights)\n",
        "        self.fc1 = nn.Linear(dim, h_dim)\n",
        "        self.fc2 = nn.Linear(h_dim*MAX_SENT, tag_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "        self.fc1.weight = nn.init.xavier_normal_(self.fc1.weight, gain = 1.0)\n",
        "        self.fc2.weight = nn.init.xavier_normal_(self.fc2.weight, gain = 1.0)\n",
        "        #self.embedding.weight = nn.init.xavier_normal_(self.embedding.weight, gain = 1.0)\n",
        "        #self.hidden = self.init_hidden()\n",
        "        #self._init_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        xshape = x.shape\n",
        "        x = self.embedding(x)\n",
        "        #print(x.shape)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        #print(x.shape)\n",
        "        x = x.reshape(x.shape[0], x.shape[1]*x.shape[2]) \n",
        "        #print(x.shape)\n",
        "        #print(x.shape)\n",
        "        x = self.fc2(x)\n",
        "        #print(x.shape)\n",
        "        x = self.sigmoid(x)\n",
        "        #x = nn.functional.log_softmax(x, dim = 1)\n",
        "        return x\n",
        "    \n",
        "    \n",
        "h_dim = 5\n",
        "layers = 1\n",
        "tag_size = 1\n",
        "    \n",
        "mlp = MLP(len(vocabToId), dim, batch_size, h_dim, layers, tag_size, weights)  \n",
        "criterion = torch.nn.BCELoss() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkKStc6sH9sm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainMLP(epochs):    \n",
        "    lossList = []\n",
        "    for e in range(epochs):\n",
        "        l = 0\n",
        "        print(\"epoch: {}\".format(e))\n",
        "        for sent, labels in train_loader:\n",
        "            mlp.zero_grad()\n",
        "            preds = mlp(sent)\n",
        "            \n",
        "            #print(preds.shape)\n",
        "            #print(labels.shape)\n",
        "            \n",
        "            #preds, labels = filterPreds(preds, labels)\n",
        "            \n",
        "            loss = criterion(preds, labels.float())\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            l += loss.item()\n",
        "        \n",
        "        lossList.append(l/len(labels))\n",
        "            \n",
        "        if e%5==0:\n",
        "            right = 0\n",
        "            count = 0\n",
        "            for devSent, devLabel in dev_loader:\n",
        "                devOutputs = mlp(devSent)\n",
        "                devPred = (devOutputs > 0.5).reshape(devOutputs.shape[0]).long()\n",
        "                count+= devLabel.size(0)\n",
        "                right+= (devPred == devLabel).sum().item()\n",
        "            accuracy = (right*100)/count\n",
        "            print(\"Iteration: {}. Loss: {}. Accuracy: {}%.\".format(e, loss.item(), accuracy))\n",
        "        \n",
        "    return lossList\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvrzOPN5H9so",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_rate = 0.001\n",
        "epochs = 25\n",
        "\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=lr_rate)\n",
        "lossListMLP = trainMLP(epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_F79qffH9sq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "right = 0\n",
        "count = 0\n",
        "for devSent, devLabel in dev_loader:\n",
        "    devOutputs = mlp(devSent)\n",
        "    devPred = (devOutputs > 0.5).reshape(devOutputs.shape[0]).long()\n",
        "    count+= devLabel.size(0)\n",
        "    right+= (devPred == devLabel).sum().item()\n",
        "accuracy = (right*100)/count\n",
        "print(\"Accuracy: {}%.\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbgyGfxOH9st",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "right = 0\n",
        "count = 0\n",
        "for devSent, devLabel in test_loader:\n",
        "    devOutputs = mlp(devSent)\n",
        "    devPred = (devOutputs > 0.5).reshape(devOutputs.shape[0]).long()\n",
        "    count+= devLabel.size(0)\n",
        "    right+= (devPred == devLabel).sum().item()\n",
        "accuracy = (right*100)/count\n",
        "print(\"Accuracy: {}%.\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkwhqoW5H9sw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}